Repositorio
# ssh cristobal@Notron
# ssh -p 5263 cfresno@notron.mine.nu
# ssh itadmin@192.168.105.61
# ssh itadmin@192.168.10.220


Host: IBM1
IP:192.168.105.61
IP:192.168.10.220
/etc/network/interfaces
# The loopback network interface
auto lo
iface lo inet loopback

# The primary network interface
auto enp2s1f0
iface enp2s1f0 inet static
	address 192.168.105.61
	netmask 255.255.255.0
	network 192.168.105.0
	broadcast 192.168.105.255
	gateway 192.168.105.254
	# dns-* options are implemented by the resolvconf package, if installed
	dns-nameservers 10.0.1.224
	dns-search inmegen.gob.mx

#DHCP
auto enp2s1f1
iface enp2s1f1 inet static
        address 192.168.21.254
        netmask 255.255.255.0
        network 192.168.21.0

Por software
RAID 0
5gb swap
RAID 1
resto /

paquetes
LAMP server
standard system utilities
OpenSSH server

mysql root 1q2w3e4r
itadmin    1q2w3e4r

Customizando
Agregar en /home/itadmin/scripts los scripts
vi auto-update.sh
#!/bin/bash

# FORMATOS
yellow='\033[1;33m'
nc='\033[0m'

#REPOSITORIOS
printf "${yellow}Actualizando repositorios...${nc}\n"
sudo apt-get -qq update
printf "${yellow}Repositorios actualizados.${nc}\n"

#PAQUETES
printf "${yellow}Actualizando paquetes...${nc}\n"
sudo apt-get -qq upgrade -y
printf "${yellow}Paquetes actualizados.${nc}\n"

#KERNEL
printf "${yellow}Actualizando KERNEL...${nc}\n"
sudo apt-get -qq dist-upgrade -y
printf "${yellow}KERNEL actualizado.${nc}\n"

#LIMPIEZA
printf "${yellow}Limpiando el sistema...${nc}\n"
sudo apt-get -qq autoclean
sudo apt-get -qq autoremove -y
sudo apt-get -qq clean

printf "${yellow}Sistema actualizado y listo.${nc}\n"

exit
chmod u+x auto-update.sh
sudo ./auto-update.sh

vi clean-kernel.sh
chmod u+x clean-kernel.sh

Agregar en /home/repositorio/scripts los scripts de sincronizacion,
con los path correspondientes y reflejarlos en el crontab
start*.sh
stop*.sh

Configuracion de lenguaje-------------------
sudo locale-gen
sudo dpkg-reconfigure locales
Elegir Us utf8

clusterssh---------------------------------
sudo apt install clusterssh
sudo vi ~/.ssh/config
Host ibm1
HostName ibm1
User itadmin

Host ibm2
HostName ibm2
User itadmin

Host ibm3
HostName ibm3
User itadmin

Host dell1
HostName dell1
User itadmin

Host dell2
HostName dell2
User itadmin

Host dell3
HostName dell3
User itadmin

sudo vi ~/.csshrc
clusters = ibms dells todas
ibms = ibm1 ibm2 ibm3
dells = dell1 dell2 dell3
todas = ibms dells

##Configurando con cssh
cssh todas

#Programas varios en todos repo + nodos#####################
sudo apt install htop iotop gdebi-core multitail clusterssh
sudo apt install gcc g++ gfortran make autoconf
sudo apt install openjdk-9-jdk-headless
sudo cpan Module::Build
sudo apt install xorg-dev xterm
sudo apt install etckeeper git-flow
sudo apt install ifenslave


#Configurando DHCP--------------------------------------
sudo apt install isc-dhcp-server
##Que placa de red usamos
sudo vi /etc/default/isc-dhcp-server
INTERFACES="enp2s1f1"
##Definiendo la red
sudo vi /etc/dhcp/dhcpd.conf

ddns-update-style none;
option domain-name-servers 10.0.1.224, 8.8.8.8, 4.4.4.4;
default-lease-time 600;
max-lease-time 7200;
log-facility local7;

# RED INTERNA
subnet 192.168.21.0 netmask 255.255.255.0 {
  range 192.168.21.100 192.168.21.200;
  option domain-name "cluster.inmegen.gob.mx";
  option routers 192.168.21.254;
  option broadcast-address 192.168.21.255;
  default-lease-time 600;
  max-lease-time 7200;
}

##Reiniciamos el servicio
sudo service isc-dhcp-server restart

#Port Forwarding para salir a internet----------------
sudo vi /etc/sysctl.conf
net.ipv4.ip_forward=1
#Reiniciamos el servicio
sudo sysctl -p
sudo /etc/init.d/procps restart

##IP Tables-------------------------------------------
Creamos el archivo iptables.sh
#!/bin/bash

iptables -F
iptables -X
iptables -Z
iptables -t nat -F

iptables -P INPUT ACCEPT
iptables -P OUTPUT ACCEPT
iptables -P FORWARD ACCEPT
iptables -t nat -P PREROUTING ACCEPT
iptables -t nat -P POSTROUTING ACCEPT

iptables -t nat -A POSTROUTING -s 192.168.21.0/24 -o enp2s1f0 -j MASQUERADE

##Lo corremos
chmod u+x iptables.sh
sudo ./iptables.sh

##Apache
sudo apt install apache2
##PXE
sudo apt install tftpd-hpa inetutils-inetd
sudo vi /etc/default/tftpd-hpa
RUN_DAEMON="yes"
OPTIONS="-l -s /var/lib/tftpboot"
TFTP_USERNAME="tftp"
TFTP_DIRECTORY="/var/lib/tftpboot"
TFTP_ADDRESS="[::]:69"
TFTP_OPTIONS="--secure"

sudo vi /etc/inetd.conf
tftp    dgram   udp    wait    root    /usr/sbin/in.tftpd /usr/sbin/in.tftpd -s /var/lib/tftpboot
sudo service tftpd-hpa restart

##Modificacmos para que dhcp tambien de pxe
sudo vi /etc/dhcp/dhcpd.conf
#Agregamos al final
allow booting;
allow bootp;
option option-128 code 128 = string;
option option-129 code 129 = text;
next-server 192.168.21.254;
filename "pxelinux.0";

sudo service isc-dhcp-server restart

##Añadimos imágenes--------------------------------------------------------
sudo mkdir /var/www/html/ubuntu_server_1604/
#copiamos todo el contenido de la iso y el instaldor de pxe donde corresponde
sudo mount /home/itadmin/iso/ubuntu-16.04.1-server-amd64.iso /mnt
sudo cp -fr /mnt/* /var/www/html/ubuntu_server_1604/
sudo cp -fr /var/www/html/ubuntu_server_1604/install/netboot/* /var/lib/tftpboot/
##agregamos estos archivos de un ubuntu corriendo
cd /usr/lib/syslinux/modules/bios/
copiamos libcom32.c32 libutil.c32 menu.c32 a /var/lib/tftpboot/ en el server de pxe
scp libcom32.c32 itadmin@192.168.10.220:/home/itadmin/
scp libutil.c32 itadmin@192.168.10.220:/home/itadmin/
scp menu.c32 itadmin@192.168.10.220:/home/itadmin/

#Agregamos el iso en el booting
sudo vi /var/lib/tftpboot/pxelinux.cfg/default
include ubuntu-installer/amd64/boot-screens/menu.cfg
default ubuntu-installer/amd64/boot-screens/vesamenu.c32
prompt 0
timeout 0
label ubuntu_64bits_server_1604
        kernel ubuntu-installer/amd64/linux
        append ks=http://192.168.21.254/ks.cfg vga=normal initrd=ubuntu-installer/adm64/initrd.gz
ramdisk_size=16432 root=/dev/rd/0 rw  --

#Clientes
#Disco 750, 5 de swap y / el resto
#Paquetes
Standard system utilities
OpenSSH server
Basic Ubuntu server

#Repositorios---------------------------------------

##APT###******************
sudo apt install apt-cacher
#como deamon
sudo vi /etc/default/apt-cacher
AUTOSTART=1
sudo vi /etc/apt-cacher/apt-cacher.conf
##Descomentar la linea
allowed_hosts = *
#restart the service
sudo /etc/init.d/apt-cacher restart
#En cada cliente agregar
sudo vi /etc/apt/apt.conf.d/90-apt-proxy
Acquire::http::Proxy "http://apt.cluster.inmegen.gob.mx:3142";
sudo apt update
sudo apt upgrade

##Clientes####################################################################
##Agregar proxy de apt
sudo vi /etc/apt/apt.conf.d/90-apt-proxy
Acquire::http::Proxy "http://192.168.21.254:3142";
sudo apt update
sudo apt upgrade
##Scripts de la carpeta itadmin@ibm1:/home/itadmin/scripts
##Configuracion de lenguaje-------------------
sudo locale-gen
sudo dpkg-reconfigure locales
Elegir es_mx y los en_us disponibles
en_us utf8 como default

##CRAN###
#Logs en /var/log/cran
cd /var/log
sudo mkdir cran
sudo chown -R itadmin:itadmin cran
#Hosting in /var/www/html/cran.inmegen.gob.mx
sudo mkdir -p /var/www/html/cran.inmegen.gob.mx
sudo chown -R itadmin:itadmin /var/www/html/cran.inmegen.gob.mx/
#Archivo de configuración en /home/itadmin/repos/startCran.sh
cd
mkdir repos
vi startCran.sh
#!/bin/bash
OF=/var/log/cran/logStartCran$(date +%Y%m%d).txt
echo "Attemping to start rsync" $(date +%Y%m%d_%H:%M:%S) &> $OF
rsync -rtlzv --delete --partial --progress cran.r-project.org::CRAN /var/www/html/cran.inmegen.gob.mx &>> $OF
##Archivo de parada stopRsync.sh
vi stopRsync.sh
#!/bin/bash
LINES=`ps aux | grep rsync | wc -l`
LOG=/var/log/cran/logStop$(date +%Y%m%d).txt
echo "Attemping to stop rsync" $(date +%Y%m%d_%H:%M:%S) &> $LOG
echo "Looking for rsync process:" $(($LINES - 1)) "found" &>> $LOG
while [  $LINES -gt 1 ]; do
        PID=`ps aux | grep rsync | grep itadmin | gawk 'NR==2{print $2}'`
        echo "Killing PID " $PID " and wait 5 seconds..." &>> $LOG
        kill -9 $PID &>> $LOG
        sleep 5s
        let LINES=`ps aux | grep rsync | wc -l`
done
echo "End!! " $(date +%Y%m%d_%H:%M:%S) &>> $LOG

chmod u+x startCran.sh
chmod u+x stopRsync.sh

#Agregar a crontab
crontab -e
0 21 * * * /home/itadmin/repos/startCran.sh
0 7 * * * /home/itadmin/repos/stopRsync.sh

##Bioconductor-----
#Sitio web
cd /var/www/html
sudo mkdir -p bioconductor.inmegen.gob.mx/packages
sudo mkdir -p bioconductor.inmegen.gob.mx/packages/3.4
sudo mkdir -p bioconductor.inmegen.gob.mx/packages/3.5
    # change these links every 6 months (with Bioc release)
cd /var/www/html/bioconductor.inmegen.gob.mx/packages/
sudo ln -s 3.4 release
sudo ln -s 3.5 devel
sudo chown -R itadmin:itadmin /var/www/html/bioconductor.inmegen.gob.mx
#Log
sudo mkdir -p /var/log/bioconductor
sudo chown -R itadmin:itadmin /var/log/bioconductor
#Archivo de configuración en /home/itadmin/repos/startBioconductorRelease.sh
cd
cd repos
vi startBioconductorRelease.sh
#!/bin/bash
OF=/var/log/bioconductor/logStartBioconductorRelease.$(date +%Y%m%d).txt
echo "Attemping to start rsync" $(date +%Y%m%d_%H:%M:%S) &> $OF
rsync -rtlzv --delete --partial --progress master.bioconductor.org::3.4 /var/www/html/bioconductor.inmegen.gob.mx/packages/3.4 &>> $OF
#Archivo de configuración en /home/itadmin/repos/startBioconductorDevel.sh
vi startBioconductorDevel.sh
#!/bin/bash
OF=/var/log/bioconductor/logStartBioconductorDevel.$(date +%Y%m%d).txt
echo "Attemping to start rsync" $(date +%Y%m%d_%H:%M:%S) &> $OF
rsync -rtlzv --delete --partial --progress master.bioconductor.org::3.5 /var/www/html/bioconductor.inmegen.gob.mx/packages/3.5 &>> $OF

chmod u+x *.sh

#Agregar a crontab
crontab -e
0 20 * * * /home/itadmin/repos/startBioconductorRelease.sh
0 20 * * * /home/itadmin/repos/startBioconductorDevel.sh

##Instalar R
cd
mkdir deb
cd deb
wget http://ibm1/cran.inmegen.gob.mx/bin/linux/ubuntu/xenial/r-base-core_3.3.2-1xenial0_amd64.deb
sudo gdebi r-base-core_3.3.2-1xenial0_amd64.deb
cd /usr/lib/R/library/base/R
sudo vi Rprofile
#Repositorio local de CRAN + Bioconductor en INMEGEN (solamente)
#Agregar las lineas en $R_HOME/library/base/R/Rprofile
options(repos = c(CRAN="http://192.168.10.220/cran.inmegen.gob.mx"))
options(BioC_mirror = "http://192.168.10.220/bioconductor.inmegen.gob.mx")
#Luego cambiar los permisos para escribir en /usr/local/lib/R/site-library
sudo chmod o+w /usr/local/lib/R/site-library
R
install.packages("e1071")
source("https://bioconductor.org/biocLite.R")
library("BiocInstaller")
biocLite()
paquetes<-c("genefu", "ggplot2", "gplots", "reshape2")
biocLite(")


##HTCondor-----------------------------------------
##Bajar de la pagina el .deb correspondiente
cd
cd deb
wget http://parrot.cs.wisc.edu//symlink/20170205031502/8/8.6/8.6.0/cc922e61b0f7a5892317195e66396aa4/condor_8.6.0-395190-ubuntu14_amd64.deb
##En todos los nodos
sudo gdebi condor_8.6.0-395190-ubuntu14_amd64.deb
#Docker para que se pase los archivos más facil
#sudo groupadd docker
sudo apt install docker.io
sudo usermod -a -G docker condor
sudo service docker restart
sudo apt install singularity
cd
mkdir source
cd source
wget https://github.com/singularityware/singularity/releases/download/2.2/singularity-2.2.tar.gz
sudo apt install make autoconf
tar -xvzf singularity-2.2.tar.gz
cd singularity-2.2
./configure --prefix=/usr
make
sudo make install
sudo apt install openjdk-9-jdk-headless

#En Todos los nodos---------------------------------
#El archivo de configuracion global sin cambios /etc/condor/condor_config
sudo /etc/init.d/condor restart
#condor_status -l | grep -i docker
######################################
cd /etc/condor/config.d/
sudo vi /etc/condor/config.d/00node
##El nodo que sera el maestro es visnu
CONDOR_HOST = visnu.cluster.inmegen.gob.mx

BIND_ALL_INTERFACES = TRUE
#Descomentar para nodo maestro
#DAEMON_LIST = STARTD, SCHEDD, COLLECTOR, NEGOTIATOR, MASTER
#Descomentar para nodo de submit
#DAEMON_LIST = MASTER, SCHEDD
Descomentar para nodo de computo
DAEMON_LIST = STARTD, MASTER
ALLOW_WRITE = *.cluster.inmegen.gob.mx

START = TRUE
SUSPEND = FALSE
CONTINUE = TRUE
PREEMPT = FALSE
KILL = FALSE

UID_DOMAIN = visnu.cluster.inmegen.gob.mx
TRUST_UID_DOMAIN = TRUE
USE_NFS = FALSE

#Si hiciera falta se pone explicito que java
#JAVA=/usr/bin/java

NUM_SLOTS = 1
NUM_SLOTS_TYPE_1 = 1
SLOT_TYPE_1 = 100%
SLOT_TYPE_1_PARTITIONABLE = TRUE

##########################################
##Actualizar /etc/hosts para que esten todos los nodos!!!
##########################################
Pag. 172
##Reiniciamos htcondor para ver que onda
sudo /etc/init.d/condor restart
##check condor
ps -aux | egrep condor_ | awk '{print $11}'
##En                        maestro   esclavo  submit master/nosubmit/nocompute
/usr/sbin/condor_master     X         X         X       X
condor_procd                X         X         X       X
condor_shared_port          X                           X
condor_collector            X                           X
condor_startd               X         X
condor_schedd               X                   X
condor_negotiator           X                           X
grep                        X         X

##Estan todas los nodos en la cola!!!! Ver en el maestro
condor_status
condor_status -master
####solo en el maestro, scheduler sirve!!!!!
condor_q

##Hello condor
vi hello.sh
#!/bin/bash
#
# hello.sh
# My very first CHTC job
#
echo "Hello CHTC from Job $1 running on `whoami`@`hostname`"
chmod o+x hello.sh
vi hello.sub
# hello.sub
# My very first HTCondor submit file
#
# Specify the HTCondor Universe (vanilla is the default and is used
#  for almost all jobs), the desired name of the HTCondor log file,
#  and the desired name of the standard error file.
#  Wherever you see $(Cluster), HTCondor will insert the queue number
#  assigned to this set of jobs at the time of submission.
universe = vanilla
log = hello.log
error = hello_$(Cluster)_$(Process).err
#
# Specify your executable (single binary or a script that runs several
#  commands), arguments, and a files for HTCondor to store standard
#  output (or "screen output").
#  $(Process) will be a integer number for each job, starting with "0"
#  and increasing for the relevant number of jobs.
executable = hello.sh
arguments = $(Process)
output = hello_$(Cluster)_$(Process).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs. The last of these lines *would* be
#  used if there were any other files needed for the executable to run.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
# transfer_input_files = file1,/absolute/pathto/file2,etc
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
request_cpus = 1
request_memory = 1GB
request_disk = 1MB
#
# Tell HTCondor to run 3 instances of our job:
queue 3
condor_submit hello.sub
##Funciono???
condor_q
condor_status
ll *.err | awk '{print $5}' | sort -u
0
rm *.err
cat *.out | awk '{print $8}' | sort -u
itadmin@dell1
itadmin@dell2
itadmin@dell3
itadmin@ibm1
itadmin@ibm2
itadmin@ibm3
rm *.out
cat hello.log | grep -i error
rm hello.log

##Agregation link por software desde ubuntu----------------------------------
#Instalar el modulo del kernel
sudo apt install ifenslave
sudo vi /etc/modules
#Agregado por CFR para link agregation
loop
lp
rtc
bonding

sudo modprobe bonding
cd /etc/network/
sudo cp interfaces interfaces.old
##Ojo en ibm enp2s1f0 enp2s1f1 mientras que en dell eno1 eno2 eno3 eno4
sudo vi interfaces
# This file describes the network interfaces available on your system
# and how to activate them. For more information, see interfaces(5).

source /etc/network/interfaces.d/*

# The loopback network interface
auto lo
iface lo inet loopback

# The primary network interface
# and configured slave to bond0
auto enp2s1f0
iface enp2s1f0 inet manual
bond-master bond0
bond-primary enp2s1f0

# The primary network interface
# and configured slave to bond0
auto enp2s1f1
iface enp2s1f1 inet manual
bond-master bond0

# bond0 is the bonded NIC and can be used like any other normal NIC.
# bond0 is configured using static network information.
auto bond0
iface bond0 inet static
address 192.168.21.102
gateway 192.168.21.254
netmask 255.255.255.0
# bond0 uses mode 6 for load and transfer balance
bond-mode 6
bond-miimon 100
bond-lacp-rate 1
bond-slaves enp2s1f0 enp2s1f1

##check OK
sudo reboot
cat /proc/net/bonding/bond0
sudo service networking status

##Programas para correr analisis
##  Aracne
##copiar ARACNE de hugo
cd /opt
sudo wget http://ibm1/ARACNE/ARACNE.src.tar.gz
sudo tar -xvzf ARACNE.src.tar.gz
sudo rm ARACNE.src.tar.gz
cd ARACNE
sudo wget http://ibm1/ARACNE/genera_sif.sh
sudo wget http://ibm1/ARACNE/clean_sif.sh
sudo chmod o+x genera_sif.sh
sudo chmod o+x clean_sif.sh
sudo apt install python-sh python-jinja2

##Desde dell2
cd
mkdir ARACNE
cd ARACNE
wget http://ibm1/ARACNE/Ms.tar.gz
tar -xvzf Ms.tar.gz
wget http://ibm1/ARACNE/parallel-aracne.tar.gz
tar -xvzf parallel-aracne.tar.gz
rm parallel-aracne.tar.gz
head ListaGenes_PBCMC_ARSYM.txt > ListaChica.txt

#aracne.sh
#!/bin/bash
#The condor temporary directory
cd ${_CONDOR_SCRATCH_DIR}

#Copy a scratch temporary ARACNE location
cp -R /opt/ARACNE .
cd ARACNE

./aracne2 $@
chmod o+x aracne.sh

#test.sub
executable = aracne.sh
error      = test.error
universe   = vanilla
log        = test.log
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files  = Basal_PBCMC_ARSYM.txt

request_cpus = 1
request_memory = 1GB
request_disk = 1MB

Arguments = -i ../Basal_PBCMC_ARSYM.txt \
            -h GTPBP6 \
            -p 1 \
            -o ../GTPBP6$(Cluster)_$(Process).adj
transfer_output_files = GTPBP6$(Cluster)_$(Process).adj
Output       = GTPBP6$(Cluster)_$(Process).log
Error        = GTPBP6$(Cluster)_$(Process).err
Queue 2



##Prueba completa
executable = aracne.sh
error      = test.error
universe   = vanilla
log        = test.log
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files  = Basal_PBCMC_ARSYM.txt

request_cpus = 1
request_memory = 1GB
request_disk = 1MB


Arguments = -i Basal_PBCMC_ARSYM.txt \
            -h GTPBP6 \
            -p 1 \
            -o GTPBP6.adj
transfer_output_files = GTPBP6.adj
Output       = GTPBP6.log
Error        = GTPBP6.err
Queue

R
genes<-as.character(read.csv(file="../ListaGenes.txt", header=FALSE)[,1])
expression<-"Basal_PBCMC_ARSYM.txt"
out<-lapply(genes, function(gen){
    texto<-paste("Arguments = -i", expression, "\\ \n",
            "\t -h", gen, "\\ \n",
            "\t -p 1 \\ \n",
            "\t -o", paste(gen, ".adj", sep=""), "\n",
    "transfer_output_files =",  paste(gen, ".adj", sep=""), "\n",
    "Output =", paste(gen, ".log", sep=""), "\n",
    "Error =", paste(gen, ".err", sep=""), "\n",
    "Queue \n \n")
    return(texto)
})
out<-do.call(c,out)

write.table(file="testCompleto.sub", out, quote=FALSE, col.names=FALSE, row.names=FALSE)









Arguments = -i ../Basal_PBCMC_ARSYM.txt \
            -h A1BG \
            -p 1 \
            -o ../A1BG.adj
transfer_output_files = A1BG.adj
Output       = A1BG.log
Error        = A1BG.err
Queue

Arguments = -i ../Basal_PBCMC_ARSYM.txt \
            -h GGACT \
            -p 1 \
            -o ../GGACT.adj
transfer_output_files = GGACT.adj
Output       = GGACT.log
Error        = GGACT.err
Queue

Arguments = -i ../Basal_PBCMC_ARSYM.txt \
            -h A2ML1 \
            -p 1 \
            -o ../A2ML1.adj
transfer_output_files = A2ML1.adj
Output       = A2ML1.log
Error        = A2ML1.err
Queue

Arguments = -i ../Basal_PBCMC_ARSYM.txt \
            -h A2M \
            -p 1 \
            -o ../A2M.adj
transfer_output_files = A2M.adj
Output       = A2M.log
Error        = A2M.err
Queue

Arguments = -i ../Basal_PBCMC_ARSYM.txt \
            -h A4GALT \
            -p 1 \
            -o ../A4GALT.adj
transfer_output_files = A4GALT.adj
Output       = A4GALT.log
Error        = A4GALT.err
Queue

Arguments = -i ../Basal_PBCMC_ARSYM.txt \
            -h AAAS \
            -p 1 \
            -o ../AAAS.adj
transfer_output_files = AAAS.adj
Output       = AAAS.log
Error        = AAAS.err
Queue

Arguments = -i ../Basal_PBCMC_ARSYM.txt \
            -h AACS \
            -p 1 \
            -o ../AACS.adj
transfer_output_files = AACS.adj
Output       = AACS.log
Error        = AACS.err
Queue

Arguments = -i ../Basal_PBCMC_ARSYM.txt \
            -h AADAT \
            -p 1 \
            -o ../AADAT.adj
transfer_output_files = AADAT.adj
Output       = AADAT.log
Error        = AADAT.err
Queue



Requirements = Machine == "notron.inmegen.gob.mx"

/var/lib/condor/execute/dir_48598


./aracne.sh -i Basal_PBCMC_ARSYM.txt -h GTPBP6 -p 1 -o /home/itadmin/ARACNE/test2/GTPBP6.adj

${_CONDOR_SCRATCH_DIR}

cd /opt/ARACNE
./aracne2 -i /home/itadmin/ARACNE/test2/Basal_PBCMC_ARSYM.txt -h GTPBP6 -p 1 -o /home/itadmin/ARACNE/test2/GTPBP6.adj
cd /home/itadmin/ARACNE/test2/



executable = aracne.sh
error      = test.error
universe   = vanilla
log        = test.log
Requirements = Machine == "notron.inmegen.gob.mx"


Arguments = -i /mnt/e/jespinal/Revision/Data/Tumor_Part_1.txt \
            -h GTPBP6 \
            -p 1 \
            -o /mnt/e/jespinal/Revision/Run/test/GTPBP6_1.adj
Output       = GTPBP6_1.log
Error        = GTPBP6_1.err
Queue





executable = aracne.sh
error      = test.error
universe   = vanilla
should_transfer_files = IF_NEEDED
when_to_transfer_output = ON_EXIT
transfer_input_files  = /opt/ARACNE.src.tar.gz,/home/itadmin/ARACNE/Basal_PBCMC_ARSYM.txt

log        = test.log



Arguments = -i Basal_PBCMC_ARSYM \
            -h GTPBP6 \
            -p 1 \
            -o GTPBP6_1.adj
transfer_output_files = GTPBP6_1.adj
Output       = GTPBP6_1.log
Error        = GTPBP6_1.err
Queue











python /home/itadmin/ARACNE/parallel-aracne/genera_condor.py --path_to_aracne2 /opt/ARACNE.src.tar.gz --expfile /home/itadmin/ARACNE/Basal_PBCMC_ARSYM.txt --probes /home/itadmin/ARACNE/ListaChica.txt --run_id test --outdir /home/itadmin/ARACNE/test --p 1

python /home/itadmin/ARACNE/parallel-aracne/genera_condor.py --aracne_tgz /opt/ARACNE.src.tar.gz --expfile /home/itadmin/ARACNE/Basal_PBCMC_ARSYM.txt --probes /home/itadmin/ARACNE/ListaChica.txt --run_id test --outdir /home/itadmin/ARACNE/test --p 1

cd /home/hachepunto/ARACNE
./aracne2 -i /mnt/e/jespinal/Revision/Data/Tumor_Part_1.txt -h A1BG -p 1 -o /mnt/e/jespinal/Revision/Run/apata/A1BG.adj

--aracne_tgz

cd /home/cristobal/ARACNE
./aracne2 -i /mnt/e/jespinal/Revision/Data/test/Expresion.txt -h GTPBP6 -o /mnt/e/jespinal/Revision/Data/test/Memo/GTPBP6_1.adj -p 1 > /mnt/e/jespinal/Revision/Data/test/Memo/GTPBP6.log 2>/mnt/e/jespinal/Revision/Data/test/Memo/GTPBP6.err

./aracne2 -i ../../M/Basal_PBCMC_ARSYM.txt -h GTPBP6 -o GTPBP6_1.adj -p 1 > GTPBP6.log 2> GTPBP6.err

##  100g
##  etc





ERRORES!!!!
dell1
No card was found in the LOM mezzanine card slot
no inicia a no ser que f1
dell3
Cambiar pila y memoria no optima
no inicia a no ser que f1



sudo vi /etc/hosts
192.168.21.254  ibm1.cluster.inmegen.gob.mx     ibm1  uno externo y otro interno                OK
192.168.21.102  ibm2.cluster.inmegen.gob.mx     ibm2  link x2                                   OK
192.168.21.106  ibm3.cluster.inmegen.gob.mx     ibm3  link x2                                   OK


192.168.21.105  dell1.cluster.inmegen.gob.mx     dell1  link x2 (le falta una placa de red)     OK
192.168.21.108  dell2.cluster.inmegen.gob.mx     dell2  link x4                                 OK
192.168.21.104  dell3.cluster.inmegen.gob.mx     dell3  link x4                                 OK



0
Joshua Haase <jihaase@inmegen.gob.mx>
10
liblua5.3-0,lua-lpeg,libtermkey1
n
y
